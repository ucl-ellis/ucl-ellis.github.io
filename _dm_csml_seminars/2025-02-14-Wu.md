---
layout: dm_csml_seminar_event
event_start_time: 2025-02-14 12:00
event_end_time: 2025-02-14 13:00
img: ellis-logo.png
alt: image-alt
join_link: https://ucl.zoom.us/j/99748820264
location: 
speaker: Lei Wu 
affiliation: Peking University
title: "Understanding Neural Network Training: A Dynamical Stability Perspective"
summary: "Training neural networks involves navigating highly non-convex and degenerate landscapes, making it challenging to understand the underlying dynamics. In this talk, we introduce a stability-based perspective to explain how stochastic gradient descent (SGD) and its variants explore these high-dimensional landscapes. This perspective provides an explanation of the well-known flat minima hypothesis: SGD converges to flat minima and flat minima generate better. In particular, our analysis can reveal the crucial roles of finite learning rate, small batch size, and the anisotropic structure of noise in help find flat minima. Finally, we present two algorithms derived from this stability perspective: one significantly accelerates the discovery of flat minima, and the other integrates seamlessly into existing deep learning frameworks to enhance large language model (LLM) pretraining."
bio: "Lei Wu is currently an assistant professor in the School of Mathematical Sciences at Peking University and his research centers on theoretical aspects of deep learning. He received his Bachelorâ€™s degree in pure mathematics from Nankai University in 2012 and a PhD degree in computational mathematics from Peking University in 2018. From November 2018 to October 2021, he worked as a postdoctoral researcher separately at Princeton University and the University of Pennsylvania."
---
